{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k8x8NxO-XIU5",
    "outputId": "91cb4b89-00cb-4c76-a2ca-151523828762"
   },
   "source": [
    "# First impressions on the problem\n",
    "\n",
    "Before I go decide on the algoritm, I said why not try to get some initial results from the simplest model possible real quick.\n",
    "\n",
    "This is super helpful when I have a problem at hand because basic ML algorithms work very well for a lot of problems with minimal complexity :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "riHuQ5NzXQlP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "EJTBxA1xXSzu",
    "outputId": "53d342fc-7e28-4767-affa-abc0b530b7b6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>cyber_label</th>\n",
       "      <th>environmental_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All rights reserved. MA23-16258 988982046\\n\\nh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revisiting our purpose and/or values statement...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amid ongoing strategic competition in a\\nmulti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Source: PwC Pulse Survey, November 2, 2022: ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Executive Summary 2\f",
       "\\nAgeing and\\nHealth Conce...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295</th>\n",
       "      <td>Source: PwC Pulse Survey, November 2, 2022: ba...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>Military-driven innovations in relevant fields...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1297</th>\n",
       "      <td>, artificial\\nintelligence, automation in all ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>Year-over-year cyberattacks continue to evolve...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>In fact, close to 40% of survey respondents sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1300 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                content  cyber_label  \\\n",
       "0     All rights reserved. MA23-16258 988982046\\n\\nh...          NaN   \n",
       "1     Revisiting our purpose and/or values statement...          NaN   \n",
       "2     Amid ongoing strategic competition in a\\nmulti...          NaN   \n",
       "3     Source: PwC Pulse Survey, November 2, 2022: ba...          NaN   \n",
       "4     Executive Summary 2\n",
       "\\nAgeing and\\nHealth Conce...          NaN   \n",
       "...                                                 ...          ...   \n",
       "1295  Source: PwC Pulse Survey, November 2, 2022: ba...          NaN   \n",
       "1296  Military-driven innovations in relevant fields...          NaN   \n",
       "1297  , artificial\\nintelligence, automation in all ...          NaN   \n",
       "1298  Year-over-year cyberattacks continue to evolve...          1.0   \n",
       "1299  In fact, close to 40% of survey respondents sa...          NaN   \n",
       "\n",
       "      environmental_issue  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       1  \n",
       "...                   ...  \n",
       "1295                    0  \n",
       "1296                    0  \n",
       "1297                    0  \n",
       "1298                    0  \n",
       "1299                    0  \n",
       "\n",
       "[1300 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/raw/train.csv\", index_col=0)\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I want to make sure I can work with the dataset, so let's explore (these are moved to the data processing nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TWb_6TumXagI",
    "outputId": "b3e25c19-4e05-48b0-c79f-95a08dc726a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cyber_label\n",
       "1.0    127\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.cyber_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-RJY08pFXxq2",
    "outputId": "780c4249-8d51-4a0e-d6e9-a201efe191c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environmental_issue\n",
       "0    1016\n",
       "1     284\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.environmental_issue.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FvyPmrP3a3nR"
   },
   "outputs": [],
   "source": [
    "train.cyber_label.fillna(0)\n",
    "train.environmental_issue.fillna(0)\n",
    "train.cyber_label = train.cyber_label.fillna(0).astype(int)\n",
    "train.environmental_issue = train.environmental_issue.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qC2pie59bTTN",
    "outputId": "6c0eea65-9422-4322-b6a5-6128932207b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       False\n",
       "1       False\n",
       "2       False\n",
       "3       False\n",
       "4       False\n",
       "        ...  \n",
       "1295    False\n",
       "1296    False\n",
       "1297    False\n",
       "1298     True\n",
       "1299    False\n",
       "Name: cyber_label, Length: 1300, dtype: bool"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"cyber_label\"] == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mjwyCCYLX4nJ",
    "outputId": "883f49a5-3c88-4bab-fe3c-830111d95e47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 92 entries, 12 to 1298\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   content              92 non-null     object\n",
      " 1   cyber_label          92 non-null     int64 \n",
      " 2   environmental_issue  92 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 2.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train[(train[\"cyber_label\"] == 1) & (train[\"environmental_issue\"] == 0)].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpBELGXVcL7V",
    "outputId": "e7f500cb-e164-4cb7-edc9-1875705959ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 249 entries, 4 to 1294\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   content              249 non-null    object\n",
      " 1   cyber_label          249 non-null    int64 \n",
      " 2   environmental_issue  249 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 7.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train[(train[\"cyber_label\"] == 0) & (train[\"environmental_issue\"] == 1)].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xj3ShXmiaV7p",
    "outputId": "3133e5c7-2f98-4c0e-c448-24cf4e90f232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 35 entries, 14 to 1218\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   content              35 non-null     object\n",
      " 1   cyber_label          35 non-null     int64 \n",
      " 2   environmental_issue  35 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train[(train[\"cyber_label\"] == 1) & (train[\"environmental_issue\"] == 1)].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aN47QyKsb2ss",
    "outputId": "8cc208e2-1ae7-427d-f1a9-75164c9b16c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 924 entries, 0 to 1299\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   content              924 non-null    object\n",
      " 1   cyber_label          924 non-null    int64 \n",
      " 2   environmental_issue  924 non-null    int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 28.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train[(train[\"cyber_label\"] == 0) & (train[\"environmental_issue\"] == 0)].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "715H6vsneI46"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train['content'], train[['cyber_label', 'environmental_issue']], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1040,), (260,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ready to train \n",
    "Let's kick start with a simple `Logistic Regression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djeU7MOxX-Tc",
    "outputId": "9613a6ac-6ec5-4184-afe5-f95110af8020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7230769230769231\n",
      "F1 Score: 0.30067933650079187\n",
      "Classification Report (multilabel eval):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.04      0.08        23\n",
      "           1       1.00      0.23      0.37        70\n",
      "\n",
      "   micro avg       1.00      0.18      0.31        93\n",
      "   macro avg       1.00      0.14      0.23        93\n",
      "weighted avg       1.00      0.18      0.30        93\n",
      " samples avg       0.07      0.06      0.06        93\n",
      "\n",
      "Prediction for the new document: [[0 0]\n",
      " [0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and a multi-label logistic regression model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultiOutputClassifier(LogisticRegression(solver='liblinear')))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report (multilabel eval):\\n\", classification_report(y_test, y_pred))\n",
    "# Example of prediction\n",
    "sample_text = [\"New document discussing climate change and cybersecurity.\", \"0\"]\n",
    "sample_prediction = pipeline.predict(sample_text)\n",
    "print(\"Prediction for the new document:\", sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((260, 2), (260, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape, y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check a random example with both labels equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "kjyzWa50YwCF",
    "outputId": "1a373b96-2080-4182-ecac-5f6dbcec81f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the new document: [[0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"content\":{\"480\":\"0\"},\"cyber_label\":{\"480\":1},\"environmental_issue\":{\"480\":1}}'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = train[(train[\"cyber_label\"] == 1) & (train[\"environmental_issue\"] == 1)].sample(1)\n",
    "\n",
    "sample_prediction = pipeline.predict(sample_text.content.to_list())\n",
    "print(\"Prediction for the new document:\", sample_prediction)\n",
    "sample_text.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "id": "xWlvv3Qhe4Bj",
    "outputId": "e7b42fd2-fe40-4c16-cb89-4d4756d17805"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>cyber_label</th>\n",
       "      <th>environmental_issue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>joke</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>got</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>is</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>kinda</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>d</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>me</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>this</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>you</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1181</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     content  cyber_label  environmental_issue\n",
       "49         3            1                    0\n",
       "111     joke            0                    0\n",
       "169        1            1                    0\n",
       "303        a            1                    0\n",
       "480        0            1                    1\n",
       "486        2            1                    1\n",
       "488      got            0                    0\n",
       "513        b            1                    1\n",
       "517        c            1                    0\n",
       "633       is            1                    0\n",
       "722    kinda            0                    1\n",
       "886        d            1                    1\n",
       "947       me            1                    1\n",
       "1015    this            1                    1\n",
       "1046     you            0                    1\n",
       "1181       0            1                    0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.content.str.len() < 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJhuskpbYvWc",
    "outputId": "2d1ae99c-b3cb-4735-884c-7088ca430f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7846153846153846\n",
      "F1 Score: 0.6940371456500489\n",
      "Classification Report (multilabel eval):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.65      0.55        23\n",
      "           1       0.74      0.74      0.74        70\n",
      "\n",
      "   micro avg       0.66      0.72      0.69        93\n",
      "   macro avg       0.61      0.70      0.64        93\n",
      "weighted avg       0.68      0.72      0.69        93\n",
      " samples avg       0.25      0.24      0.24        93\n",
      "\n",
      "Prediction for the new document: [[1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "\n",
    "# Handle missing values if any\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df['content']\n",
    "y = df[['cyber_label', 'environmental_issue']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Balancing the training data by oversampling the minority class\n",
    "# Assuming environmental_issue is the minority class\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "majority = Xy_train[Xy_train['environmental_issue'] == 0]\n",
    "minority = Xy_train[Xy_train['environmental_issue'] == 1]\n",
    "\n",
    "# Oversampling the minority\n",
    "minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "upsampled_train = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "# Re-splitting the upsampled dataset\n",
    "X_train_upsampled = upsampled_train['content']\n",
    "y_train_upsampled = upsampled_train[['cyber_label', 'environmental_issue']]\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and a multi-label logistic regression model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultiOutputClassifier(LogisticRegression(solver='liblinear', class_weight='balanced')))\n",
    "])\n",
    "\n",
    "# Train the model with the balanced dataset\n",
    "pipeline.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the model using F1 score and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report (multilabel eval):\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Example of prediction\n",
    "sample_text = [\"New document discussing climate change and cybersecurity.\"]\n",
    "sample_prediction = pipeline.predict(sample_text)\n",
    "print(\"Prediction for the new document:\", sample_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fixing the class imbalance by using balanced class weight during training, overall improvement is:\n",
    "\n",
    "Accuracy: 0.72 -> 0.78\n",
    "\n",
    "F1 Score: 0.30 -> 0.69\n",
    "\n",
    "\n",
    "We're now trying to treat this imbalance from the data directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rwVLPWa2k0Zk",
    "outputId": "8c2f7f0f-502a-4be6-dc8d-285188e6d70a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7538461538461538\n",
      "F1 Score: 0.6176949083606599\n",
      "Classification Report (multilabel eval):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "        cyber_label       0.54      0.57      0.55        23\n",
      "environmental_issue       0.62      0.66      0.64        70\n",
      "\n",
      "          micro avg       0.60      0.63      0.62        93\n",
      "          macro avg       0.58      0.61      0.60        93\n",
      "       weighted avg       0.60      0.63      0.62        93\n",
      "        samples avg       0.21      0.22      0.21        93\n",
      "\n",
      "Classification Report for 'cyber_label':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.96      0.95      0.96       237\n",
      "     Class 1       0.54      0.57      0.55        23\n",
      "\n",
      "    accuracy                           0.92       260\n",
      "   macro avg       0.75      0.76      0.75       260\n",
      "weighted avg       0.92      0.92      0.92       260\n",
      "\n",
      "F1 Score: 0.9200046366300696\n",
      "Classification Report for 'environmental_issue':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.85      0.86       190\n",
      "     Class 1       0.62      0.66      0.64        70\n",
      "\n",
      "    accuracy                           0.80       260\n",
      "   macro avg       0.75      0.75      0.75       260\n",
      "weighted avg       0.80      0.80      0.80       260\n",
      "\n",
      "F1 Score: 0.8017139479905437\n",
      "Prediction for the new document: [[1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "\n",
    "# Fill NaN values if any\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "X = df['content']\n",
    "y = df[['cyber_label', 'environmental_issue']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Balancing the training data by oversampling both minority classes\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Oversampling logic for each label\n",
    "for column in y_train.columns:\n",
    "    majority = Xy_train[Xy_train[column] == 0]\n",
    "    minority = Xy_train[Xy_train[column] == 1]\n",
    "\n",
    "    # Check if minority class needs oversampling\n",
    "    if len(minority) < len(majority):\n",
    "        minority_upsampled = resample(minority,\n",
    "                                      replace=True,\n",
    "                                      n_samples=len(majority),\n",
    "                                      random_state=42)\n",
    "        Xy_train = pd.concat([majority, minority_upsampled])\n",
    "    else:\n",
    "        Xy_train = pd.concat([majority, minority])\n",
    "\n",
    "# Re-splitting the upsampled dataset\n",
    "X_train_upsampled = Xy_train['content']\n",
    "y_train_upsampled = Xy_train[['cyber_label', 'environmental_issue']]\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and a multi-label logistic regression model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultiOutputClassifier(LogisticRegression(solver='liblinear', class_weight='balanced')))\n",
    "])\n",
    "\n",
    "# Train the model with the balanced dataset\n",
    "pipeline.fit(X_train_upsampled, y_train_upsampled)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluating the model using F1 score and classification report\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report (multilabel eval):\\n\", classification_report(y_test, y_pred, target_names=y.columns))\n",
    "\n",
    "print(\"Classification Report for 'cyber_label':\\n\", classification_report(y_test['cyber_label'], y_pred[:, 0], target_names=['Class 0', 'Class 1']))\n",
    "print(\"F1 Score:\", f1_score(y_test['cyber_label'], y_pred[:, 0], average='weighted'))\n",
    "\n",
    "print(\"Classification Report for 'environmental_issue':\\n\", classification_report(y_test['environmental_issue'], y_pred[:, 1], target_names=['Class 0', 'Class 1']))\n",
    "print(\"F1 Score:\", f1_score(y_test['environmental_issue'], y_pred[:, 1], average='weighted'))\n",
    "\n",
    "# Example of prediction\n",
    "sample_text = [\"New document discussing climate change and cybersecurity.\"]\n",
    "sample_prediction = pipeline.predict(sample_text)\n",
    "print(\"Prediction for the new document:\", sample_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's better to see the impact on each class individually, \n",
    "\n",
    "Slight performance drop, let's try a better known classifier, like Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V2rG24c_mpwJ",
    "outputId": "a1673458-96d3-4b55-cf93-58407dfdab9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n",
      "F1 Score: 0.48264811300864435\n",
      "Classification Report (multilabel eval):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "        cyber_label       0.86      0.26      0.40        23\n",
      "environmental_issue       0.81      0.37      0.51        70\n",
      "\n",
      "          micro avg       0.82      0.34      0.48        93\n",
      "          macro avg       0.83      0.32      0.45        93\n",
      "       weighted avg       0.82      0.34      0.48        93\n",
      "        samples avg       0.12      0.12      0.11        93\n",
      "\n",
      "Classification Report for 'cyber_label':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      1.00      0.96       237\n",
      "     Class 1       0.86      0.26      0.40        23\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.89      0.63      0.68       260\n",
      "weighted avg       0.93      0.93      0.91       260\n",
      "\n",
      "F1 Score: 0.9134379905808477\n",
      "Classification Report for 'environmental_issue':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.81      0.97      0.88       190\n",
      "     Class 1       0.81      0.37      0.51        70\n",
      "\n",
      "    accuracy                           0.81       260\n",
      "   macro avg       0.81      0.67      0.70       260\n",
      "weighted avg       0.81      0.81      0.78       260\n",
      "\n",
      "F1 Score: 0.7806115453174277\n",
      "Prediction for the new document: [[1 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "X = df['content']\n",
    "y = df[['cyber_label', 'environmental_issue']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handling the imbalance by oversampling each label individually\n",
    "def balance_classes(X, y):\n",
    "    Xy = pd.concat([X, y], axis=1)\n",
    "    balanced_data = pd.DataFrame()\n",
    "\n",
    "    for column in y.columns:\n",
    "        majority = Xy[Xy[column] == 0]\n",
    "        minority = Xy[Xy[column] == 1]\n",
    "\n",
    "        minority_upsampled = resample(minority,\n",
    "                                      replace=True,\n",
    "                                      n_samples=len(majority),\n",
    "                                      random_state=42)\n",
    "        balanced = pd.concat([majority, minority_upsampled])\n",
    "        balanced_data = pd.concat([balanced_data, balanced]) if not balanced_data.empty else balanced\n",
    "\n",
    "    return balanced_data['content'], balanced_data.drop('content', axis=1)\n",
    "\n",
    "X_train_balanced, y_train_balanced = balance_classes(X_train, y_train)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and a multi-label random forest model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultiOutputClassifier(RandomForestClassifier(n_estimators=100, random_state=42)))\n",
    "])\n",
    "\n",
    "# Train the model with the balanced dataset\n",
    "pipeline.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report (multilabel eval):\\n\", classification_report(y_test, y_pred, target_names=y.columns))\n",
    "\n",
    "# Evaluating the model using a detailed classification report\n",
    "print(\"Classification Report for 'cyber_label':\\n\", classification_report(y_test['cyber_label'], y_pred[:, 0], target_names=['Class 0', 'Class 1']))\n",
    "print(\"F1 Score:\", f1_score(y_test['cyber_label'], y_pred[:, 0], average='weighted'))\n",
    "\n",
    "print(\"Classification Report for 'environmental_issue':\\n\", classification_report(y_test['environmental_issue'], y_pred[:, 1], target_names=['Class 0', 'Class 1']))\n",
    "print(\"F1 Score:\", f1_score(y_test['environmental_issue'], y_pred[:, 1], average='weighted'))\n",
    "\n",
    "# Example of prediction\n",
    "sample_text = [\"New document discussing climate change and cybersecurity.\"]\n",
    "sample_prediction = pipeline.predict(sample_text)\n",
    "print(\"Prediction for the new document:\", sample_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6yBIRiE_p0TT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJ-s2nEhoYGy",
    "outputId": "023cbc99-8e31-4a57-916d-2ecd0e59c682"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Sample Accuracy: 0.75\n",
      "Accuracy Score cyber_label: 0.9307692307692308\n",
      "Accuracy Score environmental_issue: 0.8076923076923077\n",
      "Per Label avg accuracy : 0.8692307692307693\n"
     ]
    }
   ],
   "source": [
    "print(\"Full Sample Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "ac1 = accuracy_score(y_test['cyber_label'], y_pred[:, 0])\n",
    "ac2 = accuracy_score(y_test['environmental_issue'], y_pred[:, 1])\n",
    "print(\"Accuracy Score cyber_label:\", ac1)\n",
    "print(\"Accuracy Score environmental_issue:\", ac2)\n",
    "print(\"Per Label avg accuracy :\", (ac1 + ac2) / 2 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\\\n",
    "Average of both accuracies is not the same as overall accuracy score because they assume mutually correctness in order to count a sample as successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "id": "BedfdxA9inI_",
    "outputId": "ba637887-f6b7-4605-c272-b08fcadfe3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the new document: [[1 1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"content\":{\"720\":\", artificial intelligence,\\\\nautomation in all of its forms, hyper-scalable platforms, faster data transmission, quantum computing,\\\\nblockchain, digital currencies and the metaverse) and\\\\/or other market forces may outpace our\\\\norganization\\\\u2019s ability to compete and\\\\/or manage the risk appropriately, without making significant\\\\nchanges to our business model\\\\n\\\\n3 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nRegulatory changes and scrutiny may heighten, noticeably affecting the way our processes are designed\\\\nand our products or services are produced or delivered\\\\n\\\\n9 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nSubstitute products and services may arise from competitors that may enhance the customer experience\\\\nand affect the viability of our current business model and planned strategic initiatives\\\\n\\\\n11 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nEase of entrance of new competitors into the industry and marketplace or other significant changes in\\\\nthe competitive environment (such as major market concentrations due to M&A activity) may threaten\\\\nour market share\\\\n\\\\n12 \\\\u25cf \\\\u25cf \\\\u25cf 53\\\\n\\\\nhttp:\\\\/\\\\/www.protiviti.com\\\\f\\\\nStrategic Risk Issues (continued) 2032 Rank 2032 2031 2030\\\\n\\\\nSustaining customer loyalty and retention may be increasingly difficult due to evolving customer\\\\npreferences and\\\\/or demographic shifts in our existing customer base\\\\n\\\\n14 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nOur organization may not be sufficiently resilient and\\\\/or agile to manage an unexpected crisis (including\\\\na catastrophic event) significantly impacting our operations or reputation\\\\n\\\\n18 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nRapidly expanding developments in social media and platform technology innovations may significantly\\\\nimpact how we do business, interact with our customers, ensure regulatory compliance and\\\\/or manage\\\\nour brand\\\\n\\\\n19 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nGrowing focus on climate change and related ESG policies, regulations and expanding disclosure\\\\nrequirements, as well as expectations among governments, current and potential employees, and other\\\\nstakeholders about \\\\u201cgreen\\\\u201d initiatives, supply chain transparency, reward systems, and other governance\\\\nand sustainability issues, may require us to significantly alter our strategy and business model in ways\\\\nthat may be difficult for us to implement as timely as the actions of our competitors\\\\n\\\\n20 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nOpportunities for organic growth through customer acquisition and\\\\/or enhancement may be\\\\nsignificantly limited for our organization\\\\n\\\\n23 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nGrowth opportunities through acquisitions, joint ventures and other partnership activities may be\\\\ndifficult to identify and implement\\\\n\\\\n26 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nPerformance shortfalls (including lack of progress on ESG goals\\\\/expectations) may trigger activist\\\\nshareholders who seek significant changes to our organization\\\\u2019s strategic plan and vision\\\\n\\\\n28 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nOur organization may not be able to adapt its business model to embrace the evolving \\\\u201cnew normal\\\\u201d\\\\nimposed on our business by the ongoing pandemic and emerging social change\\\\n\\\\n35 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nMarket conditions imposed by and in response to COVID-19 and emerging variants, including shifts in\\\\nconsumer behavior to digital channels, may continue to impact customer demand for our core products\\\\nand services\\\\n\\\\n38 \\\\u25cf \\\\u25cf \\\\u25cf 54\\\\n\\\\nhttp:\\\\/\\\\/www.protiviti.com\\\\f\\\\nOperational Risk Issues 2032 Rank 2032 2031 2030\\\\n\\\\nOur organization\\\\u2019s succession challenges and ability to attract and retain top talent and labor amid the\\\\nconstraints of a tightening talent\\\\/labor market may limit our ability to achieve operational targets\\\\n\\\\n1 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nResistance to change in our culture may restrict our organization from making necessary adjustments to\\\\nthe business model and core operations on a timely basis\\\\n\\\\n4 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nEnsuring data privacy and compliance with growing identity protection expectations and regulations\\\\nmay require alterations demanding significant resources to restructure how we collect, store, share and\\\\nuse data to run our business\\\\n\\\\n5 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nOur existing operating processes, in-house talent, legacy IT infrastructure, lack of digital expertise and\\\\/\\\\nor insufficient digital knowledge and proficiency in the C-suite and boardroom may result in failure to\\\\nmeet performance expectations related to quality, time to market, cost and innovation as well as our\\\\ncompetitors, including those that are either \\\\u201cborn digital\\\\u201d or investing heavily to leverage technology for\\\\ncompetitive advantage\\\\n\\\\n6 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nInability to utilize data analytics and \\\\u201cbig data\\\\u201d to achieve market intelligence, gain insights on the\\\\ncustomer experience, and increase productivity and efficiency may significantly affect our management\\\\nof core operations and strategic plans\\\\n\\\\n7 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nOur organization may not be sufficiently prepared to manage cyber threats such as ransomware and\\\\nother attacks that have the potential to significantly disrupt core operations and\\\\/or damage our brand\\\\n\\\\n13 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nThird-party risks arising from our reliance on outsourcing and strategic sourcing arrangements,\\\\necosystem partners, IT vendor contracts, and other partnerships\\\\/joint ventures to achieve operational\\\\nand go-to-market goals may prevent us from meeting organizational targets or impact our brand image\\\\n\\\\n15 \\\\u25cf \\\\u25cf \\\\u25cf\\\\n\\\\nOur organization\\\\u2019s culture may not sufficiently encourage the timely identification and escalation of risk\\\\nissues and market opportunities that have the potential to significantly affect our core operations and\\\\nachievement of strategic objectives\\\\n\\\\n16 \\\\u25cf \\\\u25cf \\\\u25cf 55\\\\n\\\\nhttp:\\\\/\\\\/www.protiviti.com\\\\f\\\\nOperational Risk Issues (continued) 2032 Rank 2032 2031 2030\\\\n\\\\nThe rising threat associated with catastrophic natural disasters and weather phenomena (e.g.\"},\"cyber_label\":{\"720\":1},\"environmental_issue\":{\"720\":1}}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = train[(train[\"cyber_label\"] == 1) & (train[\"environmental_issue\"] == 1)].sample(1)\n",
    "\n",
    "sample_prediction = pipeline.predict(sample_text.content.to_list())\n",
    "print(\"Prediction for the new document:\", sample_prediction)\n",
    "sample_text.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSNqPqj6jkoX"
   },
   "source": [
    "I'd like to try two models to see if we can get better classification than multi-label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DhBDNFgurvD7",
    "outputId": "9c3704b0-5243-4309-839f-69a09fdde4da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for cyber_label:\n",
      "Accuracy: 0.9346153846153846\n",
      "Recall: 0.30434782608695654\n",
      "Precision: 0.875\n",
      "F1 Score: 0.45161290322580644\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97       237\n",
      "           1       0.88      0.30      0.45        23\n",
      "\n",
      "    accuracy                           0.93       260\n",
      "   macro avg       0.91      0.65      0.71       260\n",
      "weighted avg       0.93      0.93      0.92       260\n",
      "\n",
      "Model Performance for environmental_issue:\n",
      "Accuracy: 0.8192307692307692\n",
      "Recall: 0.4857142857142857\n",
      "Precision: 0.7555555555555555\n",
      "F1 Score: 0.591304347826087\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88       190\n",
      "           1       0.76      0.49      0.59        70\n",
      "\n",
      "    accuracy                           0.82       260\n",
      "   macro avg       0.79      0.71      0.74       260\n",
      "weighted avg       0.81      0.82      0.81       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "\n",
    "# Handling missing values\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df[['cyber_label', 'environmental_issue']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Function to balance each class using resampling\n",
    "def balance_classes(X, y):\n",
    "    # Convert sparse matrix to DataFrame if necessary\n",
    "    if sparse.issparse(X):\n",
    "        X = pd.DataFrame(X.toarray())  # Note this could be memory-intensive with large datasets\n",
    "\n",
    "    # Reset index if X or y are series or dataframes to ensure alignment\n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    # Concatenate X and y along axis=1\n",
    "    Xy = pd.concat([X, y], axis=1)\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    majority = Xy[Xy.iloc[:, -1] == 0]\n",
    "    minority = Xy[Xy.iloc[:, -1] == 1]\n",
    "\n",
    "    # Resample the minority class\n",
    "    if len(minority) < len(majority):\n",
    "        minority_upsampled = resample(minority, replace=True, n_samples=len(majority), random_state=42)\n",
    "        balanced_data = pd.concat([majority, minority_upsampled])\n",
    "    else:\n",
    "        balanced_data = Xy\n",
    "\n",
    "    # Return balanced X and y\n",
    "    return balanced_data.iloc[:, :-1], balanced_data.iloc[:, -1]\n",
    "\n",
    "\n",
    "# Models for each label\n",
    "def train_model(X_train, y_train, X_test, y_test, n_estimators=100):\n",
    "    # Balance the training dataset\n",
    "    X_train_balanced, y_train_balanced = balance_classes(X_train, y_train)\n",
    "\n",
    "    # Train the model\n",
    "    model = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Train and evaluate model for cyber_label\n",
    "print(\"Model Performance for cyber_label:\")\n",
    "train_model(X_train_tfidf, y_train['cyber_label'], X_test_tfidf, y_test['cyber_label'], n_estimators=80)\n",
    "\n",
    "# Train and evaluate model for environmental_issue\n",
    "print(\"Model Performance for environmental_issue:\")\n",
    "train_model(X_train_tfidf, y_train['environmental_issue'], X_test_tfidf, y_test['environmental_issue'], n_estimators=43)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y0V4T9jWvBLd"
   },
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A5-BSDNzsBe"
   },
   "source": [
    "SVM model is a pretty good classifier, let's give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riV-RzmovIbP",
    "outputId": "9a10d947-a422-4507-a57d-f7efb4171d16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for cyber_label:\n",
      "Accuracy: 0.9115384615384615\n",
      "Recall: 0.5652173913043478\n",
      "Precision: 0.5\n",
      "F1 Score: 0.5306122448979592\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       237\n",
      "           1       0.50      0.57      0.53        23\n",
      "\n",
      "    accuracy                           0.91       260\n",
      "   macro avg       0.73      0.76      0.74       260\n",
      "weighted avg       0.92      0.91      0.91       260\n",
      "\n",
      "Model Performance for environmental_issue:\n",
      "Accuracy: 0.8192307692307692\n",
      "Recall: 0.6285714285714286\n",
      "Precision: 0.676923076923077\n",
      "F1 Score: 0.6518518518518519\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.89      0.88       190\n",
      "           1       0.68      0.63      0.65        70\n",
      "\n",
      "    accuracy                           0.82       260\n",
      "   macro avg       0.77      0.76      0.76       260\n",
      "weighted avg       0.82      0.82      0.82       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "\n",
    "\n",
    "# Handling missing values\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], df[['cyber_label', 'environmental_issue']], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "def balance_classes(X, y):\n",
    "    # Ensure y is a DataFrame with correct indices\n",
    "    y = pd.DataFrame(y).reset_index(drop=True)\n",
    "    # Determine indices of majority and minority classes\n",
    "    majority_idx = y[y.iloc[:, 0] == 0].index\n",
    "    minority_idx = y[y.iloc[:, 0] == 1].index\n",
    "\n",
    "    # Resample the minority class\n",
    "    if len(minority_idx) < len(majority_idx):\n",
    "        minority_upsampled_idx = resample(minority_idx, replace=True, n_samples=len(majority_idx), random_state=42)\n",
    "        new_indices = pd.Index(majority_idx.tolist() + minority_upsampled_idx.tolist())\n",
    "    else:\n",
    "        new_indices = pd.Index(majority_idx.tolist() + minority_idx.tolist())\n",
    "\n",
    "    # Return balanced X and y using the new indices\n",
    "    return X[new_indices], y.iloc[new_indices].values.ravel()\n",
    "\n",
    "\n",
    "# Models for each label\n",
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    # Balance the training dataset\n",
    "    X_train_balanced, y_train_balanced = balance_classes(X_train, y_train)\n",
    "\n",
    "    # Train the model\n",
    "    model = SVC(kernel='linear', C=1, probability=True, random_state=42)\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Train and evaluate model for cyber_label\n",
    "print(\"Model Performance for cyber_label:\")\n",
    "train_model(X_train_tfidf, y_train['cyber_label'], X_test_tfidf, y_test['cyber_label'])\n",
    "\n",
    "# Train and evaluate model for environmental_issue\n",
    "print(\"Model Performance for environmental_issue:\")\n",
    "train_model(X_train_tfidf, y_train['environmental_issue'], X_test_tfidf, y_test['environmental_issue'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me check the impact if we get back to multi-label instead of two-model clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ya7hdEiqvIWB",
    "outputId": "85307eb1-78df-47e9-fa80-077b826caa33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7461538461538462\n",
      "F1 Score: 0.5774244360799397\n",
      "Classification Report (multilabel eval):\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "        cyber_label       0.53      0.43      0.48        23\n",
      "environmental_issue       0.66      0.57      0.61        70\n",
      "\n",
      "          micro avg       0.62      0.54      0.58        93\n",
      "          macro avg       0.59      0.50      0.54        93\n",
      "       weighted avg       0.62      0.54      0.58        93\n",
      "        samples avg       0.19      0.18      0.18        93\n",
      "\n",
      "Classification Report for 'cyber_label':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.95      0.96      0.95       237\n",
      "     Class 1       0.53      0.43      0.48        23\n",
      "\n",
      "    accuracy                           0.92       260\n",
      "   macro avg       0.74      0.70      0.72       260\n",
      "weighted avg       0.91      0.92      0.91       260\n",
      "\n",
      "F1 Score: 0.9117093506214846\n",
      "Classification Report for 'environmental_issue':\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.85      0.89      0.87       190\n",
      "     Class 1       0.66      0.57      0.61        70\n",
      "\n",
      "    accuracy                           0.80       260\n",
      "   macro avg       0.75      0.73      0.74       260\n",
      "weighted avg       0.80      0.80      0.80       260\n",
      "\n",
      "F1 Score: 0.7993771765235099\n",
      "Prediction for the new document: [[1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/giyaseddin/miniconda3/envs/text-classification-dtse-challenge/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "df = train.copy()\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "X = df['content']\n",
    "y = df[['cyber_label', 'environmental_issue']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Handling the imbalance by oversampling each label individually\n",
    "def balance_classes(X, y):\n",
    "    Xy = pd.concat([X, y], axis=1)\n",
    "    balanced_data = pd.DataFrame()\n",
    "\n",
    "    for column in y.columns:\n",
    "        majority = Xy[Xy[column] == 0]\n",
    "        minority = Xy[Xy[column] == 1]\n",
    "\n",
    "        minority_upsampled = resample(minority,\n",
    "                                      replace=True,\n",
    "                                      n_samples=len(majority),\n",
    "                                      random_state=42)\n",
    "        balanced = pd.concat([majority, minority_upsampled])\n",
    "        balanced_data = pd.concat([balanced_data, balanced]) if not balanced_data.empty else balanced\n",
    "\n",
    "    return balanced_data['content'], balanced_data.drop('content', axis=1)\n",
    "\n",
    "X_train_balanced, y_train_balanced = balance_classes(X_train, y_train)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer and a multi-label random forest model\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
    "    ('classifier', MultiOutputClassifier(SVC(kernel='linear', C=1, probability=True, random_state=42)))\n",
    "])\n",
    "\n",
    "# Train the model with the balanced dataset\n",
    "pipeline.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Classification Report (multilabel eval):\\n\", classification_report(y_test, y_pred, target_names=y.columns))\n",
    "\n",
    "# Evaluating the model using a detailed classification report\n",
    "print(\"Classification Report for 'cyber_label':\\n\", classification_report(y_test['cyber_label'], y_pred[:, 0], target_names=['Class 0', 'Class 1']))\n",
    "print(\"F1 Score:\", f1_score(y_test['cyber_label'], y_pred[:, 0], average='weighted'))\n",
    "\n",
    "print(\"Classification Report for 'environmental_issue':\\n\", classification_report(y_test['environmental_issue'], y_pred[:, 1], target_names=['Class 0', 'Class 1']))\n",
    "print(\"F1 Score:\", f1_score(y_test['environmental_issue'], y_pred[:, 1], average='weighted'))\n",
    "\n",
    "# Example of prediction\n",
    "sample_text = [\"New document discussing climate change and cybersecurity.\"]\n",
    "sample_prediction = pipeline.predict(sample_text)\n",
    "print(\"Prediction for the new document:\", sample_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eR3O-1rRvISp"
   },
   "source": [
    "Now that we tried a couple of these models, it's clear that the data is not super easy to classify. \\\n",
    "We need more experiments to get better results."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
